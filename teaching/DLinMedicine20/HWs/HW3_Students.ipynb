{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning in Medicine\n",
    "### BMSC-GA 4493, BMIN-GA 3007 \n",
    "### Homework 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning Objectives**:\n",
    "\n",
    "1. Transferred Learning\n",
    "2. GAN\n",
    "3. Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instruction** \n",
    "\n",
    "1. If you need to write mathematical terms, you can type your answeres in a Markdown Cell via LaTex. See: <a href=\"https://stackoverflow.com/questions/13208286/how-to-write-latex-in-ipython-notebook\">here</a> if you have issues with writing equations. To see basic LaTex notation see: <a href=\"https://en.wikibooks.org/wiki/LaTeX/Mathematics\"> here </a>.\n",
    "\n",
    "2. Upload and Submit your final jupyter notebook file in <a href='http://newclasses.nyu.edu '>newclasses.nyu.edu</a>\n",
    "\n",
    "3. Deadline: Thursday April 16th 2020 (3pm)\n",
    "\n",
    "4. Questions and Clarification: <a href=\"https://piazza.com/nyumc.org/spring2020/bmscga4493andbminga3007/home\"> Class Piazza</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Literature Review: DeepPatient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read this paper:\n",
    "\n",
    "Riccardo Miotto, Li Li, Brian A. Kidd & Joel T. Dudley, \"Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records\" Scientific Reports, 2016 https://www.nature.com/articles/srep26094.pdf\n",
    "\n",
    "We are interested in understanding the task, the methods that is proposed in this publication, technical aspects of the implementation, and possible future work. After you read the full article answer the following questions. Describe you answers in your own words.  \n",
    "\n",
    "### 1.1.\n",
    "What type of learning algorithm is used (supervised, semi-supervised or unsupervised)? What is the reason for selecting this type of learning algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.\n",
    "What type of neural network architecture is used in the paper? What is the reason for selecting this type of network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.\n",
    "What is the loss function? Whay kind of approaches are used to improve model generalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.\n",
    "How many hidden layers and units does DeepPatient has? What type of activation function is used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.\n",
    "What are the evaluation metrics used for model comparison? Explain why those metrics were chosen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.\n",
    "How did the authors decide on using specific number of hidden layers in the article? What other architectures would you try?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Transfer learning for disease classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the howework, we will revisit the disease classification task we worked on HW2. As opposed to developing our own CNN network as we did on HW2, in this HW we are interested in using transfer learning for the disease classification task.  we will use ResNet50 model to achieve this goal. Here is the link for the ResNet paper: https://arxiv.org/pdf/1512.03385.pdf You will use ResNet50 model as a fixed feature extractor in this question. \n",
    "\n",
    "As a reminder: we focused on classifiying the lung disease using chest x-ray dataset provided by NIH (https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community). Please go over the following paper for the details of the dataset: https://arxiv.org/pdf/1705.02315.pdf \n",
    "\n",
    "You need to use HPC for training part of this question, as your computer's CPU will not be fast enough to compute learning iterations. In case you use HPC, please have your code/scripts uploaded under the questions and provide the required plots and tables there as well. Data is available in HPC under /beegfs/ga4493/data/HW2 folder. We are interested in classifying infiltration, pneumothorax, cardiomegaly and *not*(infiltration OR pneumothorax OR cardiomegaly) cases. By saying so we have 4 classes that we want to identify by modelling a deep CNN.\n",
    "\n",
    "Due to time limitations you do not need to your models using the whole dataset, we will use HW2_RandomtrainSet.csv, HW2_testSet.csv and HW2_RandomvalidationSet.csv provided under /beegfs/ga4493/data/HW2 folder for defining train, test and validation set samples that are generated in HW2 Q3.1.\n",
    "In these .csv files 4 classes were defined as :\n",
    "- 1 infiltration\n",
    "- 2 pneumothorax\n",
    "- 3 cardiomegaly\n",
    "- 0 for all other diseases (doesnt have infiltration OR pneumothorax OR cardiomegaly) or NoFinding\n",
    "\n",
    "### 2.1. Define dataloaders for train, test and validations sets\n",
    "We used Dataset class defined in HW2. Here, we have a copy of it. You need to make necessary changes to make the dataset class is capable of feeding the X-ray dataset into resnet model for transfer learning. Remember X-ray data has grayscale images of 1024x1024 pixels and resnet50 model requires RGB images of size 224x224. Add necessary code to the specified locations: \n",
    "\n",
    "### 2.1.a. \n",
    "Define data transforms required for train and validation/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage import io\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from skimage import color\n",
    "\n",
    "train_transform = ### Provide your code here ###\n",
    "\n",
    "validation_transform = ### Provide your code here ###\n",
    "\n",
    "class ChestXrayDataset(Dataset):\n",
    "    \"\"\"Chest X-ray dataset from https://nihcc.app.box.com/v/ChestXray-NIHCC.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file filename information.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.data_frame.iloc[idx, 0])\n",
    "        \n",
    "        #some cases io.imread brings more channels than 1 due to bitsize issues\n",
    "        image = io.imread(img_name)\n",
    "        if len(image.shape) > 2 and image.shape[2] == 4:\n",
    "            image = image[:,:,0]\n",
    "\n",
    "        # replicate the image into 3 RGB channels\n",
    "        image=np.repeat(image[None,...],3,axis=0)\n",
    "        \n",
    "        image_class = self.data_frame.iloc[idx, -1]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        sample = {'x': image, 'y': image_class}\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.b.\n",
    "Define train, validation and test dataloaders loaders using the dataset class defined in Q.3.1.a and .csv files: HW2_RandomtrainSet.csv, HW2_testSet.csv and HW2_RandomvalidationSet.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the data loaders with emprical batch size \n",
    "BATCH_SIZE = 16\n",
    "## data loaders\n",
    "chestXray_TrainData = ChestXrayDataset(### Provide your code here ###)\n",
    "train_loader =  ### Provide your code here ###\n",
    "\n",
    "chestXray_ValidationData = ChestXrayDataset(### Provide your code here ###)\n",
    "validation_loader = ### Provide your code here ###\n",
    "\n",
    "chestXray_TestData = ChestXrayDataset(### Provide your code here ###)\n",
    "test_loader = ### Provide your code here ###\n",
    "\n",
    "dataset_sizes = {'train': len(chestXray_TrainData), 'val': len(chestXray_ValidationData)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions --> You dont need to add/remove anything here. Functions implemented on HW2 will be used here. if you want, you can use your own functions as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(model, criterion, optimizer, num_epochs=25, trainVal=['train','val'],verbose=True):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    loss2plot = np.zeros([2,num_epochs])\n",
    "    acc2plot  = np.zeros([2,num_epochs])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if verbose:\n",
    "            print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "            print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in trainVal:\n",
    "            if phase == 'train':\n",
    "                imageLoader = train_loader\n",
    "            else:\n",
    "                imageLoader = validation_loader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for sample_batched in imageLoader:\n",
    "                # get the inputs\n",
    "                #print(sample_batched)\n",
    "                inputs = sample_batched['x']\n",
    "                labels = sample_batched['y']\n",
    "\n",
    "                # wrap them in Variable\n",
    "                if use_gpu:\n",
    "                    inputs = Variable(inputs).type(torch.FloatTensor).cuda()\n",
    "                    labels = Variable(labels).type(torch.LongTensor).cuda()\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs).type(torch.FloatTensor), Variable(labels).type(torch.LongTensor)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.data[0] * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                #print(labels.data,preds,preds == labels.data,outputs.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            if verbose:\n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == 'train':\n",
    "                loss2plot[0,epoch] = epoch_loss\n",
    "                acc2plot[0,epoch] = epoch_acc\n",
    "            else:\n",
    "                loss2plot[1,epoch] = epoch_loss\n",
    "                acc2plot[1,epoch] = epoch_acc\n",
    "                    \n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if verbose:\n",
    "            print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    for phase in trainVal:\n",
    "        if phase == 'train':\n",
    "            idx=0\n",
    "        else:\n",
    "            idx=1\n",
    "            \n",
    "        fig = plt.figure()\n",
    "        \n",
    "    \n",
    "        a = fig.add_subplot(2,2,2*idx+1)\n",
    "        plt.plot(loss2plot[idx,:])\n",
    "        plt.title('Loss per epoch for ' + phase)\n",
    "        #plt.suptitle('Curves for ' + phase)\n",
    "\n",
    "        a = fig.add_subplot(2,2,2*idx+2)\n",
    "        plt.plot(acc2plot[idx,:])\n",
    "        plt.title('Accuracy per epoch for ' + phase)\n",
    "        plt.show()\n",
    "\n",
    "        #plt.plot(loss2plot[idx,:]);plt.title('Loss per epoch for ' + phase); plt.show()\n",
    "        #plt.plot(acc2plot[idx,:]);plt.title('Accuracy per epoch for ' + phase); plt.show()\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# This is the place we predict the disease from a model trained, output for this function is \n",
    "#the target values and probabilty of each image having a disease \n",
    "import torch.nn.functional as F\n",
    "def inference(model_ft,loader):\n",
    "    use_gpu = 1\n",
    "    model_ft.eval()\n",
    "    whole_output =[]\n",
    "    whole_target = []\n",
    "\n",
    "    for valData in loader:\n",
    "        data = valData['x']\n",
    "        target = valData['y']\n",
    "        if use_gpu:\n",
    "            data = Variable(data,volatile=True).type(torch.FloatTensor).cuda()\n",
    "            target = Variable(target,volatile=True).type(torch.LongTensor).cuda()\n",
    "        else:\n",
    "            data, target = Variable(data,volatile=True).type(torch.FloatTensor), Variable(target,volatile=True).type(torch.LongTensor)\n",
    "\n",
    "        output =F.softmax(model_ft(data),dim=1)\n",
    "        whole_output.append( output.cpu().data.numpy())\n",
    "        whole_target.append( valData['y'].numpy())\n",
    "\n",
    "    whole_output = np.concatenate(whole_output)\n",
    "    whole_target = np.concatenate(whole_target)\n",
    "\n",
    "    y_score = whole_output\n",
    "    y_target = label_binarize(whole_target, classes=[0, 1, 2, 3])\n",
    "    \n",
    "    return y_score, y_target\n",
    "\n",
    "# this function AUC of ROC for each disease seperately and also macro and micro averages,\n",
    "# we will use macro average to compare different models we will train. \n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from scipy import interp\n",
    "from itertools import cycle\n",
    "\n",
    "def get_AUC(y_score, y_target,plotROC=False):\n",
    "    n_classes = y_score.shape[1]\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_target[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_target.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    \n",
    "    if plotROC:\n",
    "        lw = 2\n",
    "        # Plot all ROC curves\n",
    "        plt.figure()\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                 label='micro-average ROC curve (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"micro\"]),\n",
    "                 color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                 label='macro-average ROC curve (area = {0:0.2f})'\n",
    "                       ''.format(roc_auc[\"macro\"]),\n",
    "                 color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "        colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "        for i, color in zip(range(n_classes), colors):\n",
    "            plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "                     label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "                     ''.format(i, roc_auc[i]))\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('Some extension of Receiver operating characteristic to multi-class')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "    return roc_auc\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Loading the pre-trained CNN model, training the model and analyzing results (TL as a feature extractor) \n",
    "\n",
    "### 2.2.a.\n",
    "Since now we can import images using dataloaders, next step is to load the pretrained resnet50 network from torchvision.models and prepare it for transfer learning for using resnet model as a feature extractor (see slide #12 for Lecture 12). Write the code for designing resnet50 architecture and loading the weights from ImageNet trained network. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.b.\n",
    "Define the loss and the optimizer you want to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.c.\n",
    "Train the model for 100 epochs and save the weights of the best model using the validation loss. Plot train and validation loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.d.\n",
    "Plot ROC curve of the best model using the dataloader of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.e.\n",
    "Compute the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Loading the pre-trained CNN model, training the model and analyzing results (TL with fine-tuning) \n",
    "\n",
    "### 2.3.a.\n",
    "Perform a transfer learning using resnet50 model as a fine-tuning base model (see slide #13 for Lecture 12).\n",
    "Write the code for designing resnet50 architecture and loading the weights from ImageNet trained network.                                                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.b.\n",
    "Define the loss and the optimizer you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.c.\n",
    "Train the model for 100 epochs and save the weights of the best model using the validation loss. Plot train and validation loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.d.\n",
    "Plot ROC Curve and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.\n",
    "Descibe your findings using two different TL approaches that you implemented in Q2.2. and Q2.3. for example what are the differences in terms of speed, accuracy, and so on. Was TL successfull classifying lung diseases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.\n",
    "In the view of your findings, describe transfer learning approaches we used in this HW in terms of data size and emprical observation domain specifics (Hint: Checkslide # 16 on Lecture #12). What can you do better to improve transfer learning? Propose changes to the current TL strategies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. Bonus Question\n",
    "Implement the changes you proposed in Q2.5 and analyze your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
